Para tener un dump HTML de Wikipedia el mejor método que encontramos es
utilizar el listado de todos los artículos disponibles y descargarlos
utilizando el script 'scraper.py'. Para esto hay que seguir los siguientes
pasos:

1) Descargar el listado actualizado:
$wget http://dumps.wikimedia.org/eswiki/latest/eswiki-latest-all-titles-in-ns0.gz

2) Descomprimirlo
$gunzip eswiki-latest-all-titles-in-ns0.gz

3) Si se desean incluir los artículos que se encuentran desntro de Namespaces
(por ejemplo los Anexos) ejecutar $python listar_articulos_en_namespaces.py
y se creará un archivo 'articulos_en_namespaces.txt' listo para usar.  Para
elegir cuáles Namespaces se desean incluir se debe editar la lista SPACES
dentro del script.

4-Opcional)
 a) Se puede comprobar cuántos artículos tiene cada listado:
$wc -l articulos_en_namespaces.txt
176779 articulos_en_namespaces.txt
$wc -l eswiki-latest-all-titles-in-ns0
1933961 eswiki-latest-all-titles-in-ns0
 b) Para el caso de CdPedia se tienen 7 artículos que se deben incluir
$wc -l include.txt
7 include.txt

5) A partir de los listados, generamos uno solo:
$cat eswiki-latest-all-titles-in-ns0 > todos_los_articulos
$cat include.txt >> todos_los_articulos
$cat articulos_en_namespaces.txt >> todos_los_articulos
$wc -l todos_los_articulos
2110747 todos_los_articulos

6) Ejecutamos scraper.py con 2 argumentos: el listado completo y el path base
donde se guardaras los artículos. Para el dump completo hoy se requieren de 45G
disponibles en el disco. (Nota: este programa demorará muchas horas en
descargar todos los artículos):
$python scraper.py todos_los_articulos /var/w/articulos

El script mostrará en pantalla una línea similar a esta:
TOTAL=2104825   BIEN=2103573    MAL=1252        velocidad = 14.51 art/s

TOTAL: Consiste en el total de artículos procesados.
BIEN: Artículos que pudieron descargarse correctamente
MAL: Artículos que no se encontraron
velocidad: sencillamente la división entre los artículos descargados y el
           tiempo demorado hasta el momento.

El script al detenerse guardará la posición actual (en posicion.txt) por lo que
puede continuarse en otro momento.

También hay que revisar si cambiaron los "Términos de uso":

1. Ir a la página http://wikimediafoundation.org/wiki/T%C3%A9rminos_de_Uso

2. Ver si la última vez que se modificó (ver el footer) es "2 August 2012, at 13:46"

3. Si sí, listo, no hay nada que actualizar.

4. Si no, bajar esa página a mano, guardarla en resources/institucional/Términos_de_uso.htm

5. Aplicarle la misma limpieza que la función 'extract_content' en utilities/scraper.py.

6. Borrar todos los otros lenguajes (entre un <ul> y </ul>), si los hubiera

7. Agregar un comentario class  InformationBox como el que ya está que informe que...

    Esta página fue descargada de
    <a href="http://wikimediafoundation.org/wiki/T%C3%A9rminos_de_Uso">
    esta dirección externa</a> el ***FECHA***, por favor recurrir a la versión
    on-line para asegurarse de estar leyendo la versión más actualizada. (mensaje
    agregado para la CDPedia).

8. Corregir ahí mismo el ícono de la "i" azulada para que apunte a
   "/misc/32px-Information_icon.svg.png"


Cualquier duda, se puede consultar en el grupo  de CDPedia ( cdpedia@googlegroups.com )
